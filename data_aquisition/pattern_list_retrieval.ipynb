{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3447218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "BASE_URL = \"https://api.ravelry.com\"\n",
    "\n",
    "# The os.getenv() calls will now find the variables loaded from your .env file\n",
    "RAVELRY_ACCESS_KEY = os.getenv('RAVELRY_ACCESS_KEY')\n",
    "RAVELRY_PERSONAL_KEY = os.getenv('RAVELRY_PERSONAL_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40d1bb58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Access Key Loaded: read-d4086974ad193fe02828dd97c21b9560\n",
      "Personal Key Loaded: Eq5JjrVDcMu4Ji01Y2aQ9bMh4gtUpr1JoSYsG7Ri\n"
     ]
    }
   ],
   "source": [
    "# --- ADD THIS DEBUGGING CODE ---\n",
    "print(f\"Access Key Loaded: {RAVELRY_ACCESS_KEY}\")\n",
    "print(f\"Personal Key Loaded: {RAVELRY_PERSONAL_KEY}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a94c762",
   "metadata": {},
   "source": [
    "Writing a function to test grabbing different attributes that may be useful in analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfd86a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_patterns(query, max_pages=100):\n",
    "    \"\"\"\n",
    "    Searches for patterns on Ravelry with a specific query,\n",
    "    handling pagination to retrieve a large number of results.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search term for patterns (e.g., \"sweater\").\n",
    "        max_pages (int): The maximum number of pages to fetch. \n",
    "                         Set to None to fetch all pages.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame of patterns that match the criteria.\n",
    "    \"\"\"\n",
    "    endpoint = f\"{BASE_URL}/patterns/search.json\"\n",
    "    all_patterns = []\n",
    "    page = 1\n",
    "    \n",
    "    print(f\"Starting pattern search for: '{query}'\")\n",
    "\n",
    "    while True:\n",
    "        # Parameters for the API request, including the current page\n",
    "        params = {\n",
    "            \"query\": query,\n",
    "            \"page_size\": 100,\n",
    "            \"page\": page\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            print(f\"Fetching page {page}...\")\n",
    "            response = requests.get(endpoint, auth=(RAVELRY_ACCESS_KEY, RAVELRY_PERSONAL_KEY), params=params)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            data = response.json()\n",
    "            patterns_on_page = data.get('patterns', [])\n",
    "            \n",
    "            if not patterns_on_page:\n",
    "                print(\"No more patterns found. Ending search.\")\n",
    "                break  # Exit the loop if a page is empty\n",
    "\n",
    "            all_patterns.extend(patterns_on_page)\n",
    "            \n",
    "            # Check paginator to see if we are on the last page\n",
    "            paginator = data.get('paginator', {})\n",
    "            if paginator.get('last_page') == page:\n",
    "                print(\"Reached the last page of results.\")\n",
    "                break\n",
    "\n",
    "            # Check if we have reached the user-defined max_pages limit\n",
    "            if max_pages is not None and page >= max_pages:\n",
    "                print(f\"Reached max_pages limit of {max_pages}.\")\n",
    "                break\n",
    "                \n",
    "            page += 1\n",
    "            \n",
    "            # --- RATE LIMITING ---\n",
    "            # Wait for 1 second before the next request to be respectful to the API\n",
    "            time.sleep(1)\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            return None\n",
    "    \n",
    "    # Process all collected patterns into a DataFrame\n",
    "    patterns_data = []\n",
    "    for pattern in all_patterns:\n",
    "        patterns_data.append({\n",
    "            'Name': pattern.get('name'),\n",
    "            'Designer': pattern.get('designer', {}).get('name'),\n",
    "            'ID': pattern.get('id'),\n",
    "            'URL': f\"https://www.ravelry.com/patterns/library/{pattern.get('permalink')}\",\n",
    "            'Free': pattern.get('free')\n",
    "        })\n",
    "\n",
    "    print(f\"Total patterns collected: {len(patterns_data)}\")\n",
    "    return pd.DataFrame(patterns_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "079e745a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting pattern search for: 'sweater'\n",
      "Fetching page 1...\n",
      "Fetching page 2...\n",
      "Fetching page 3...\n",
      "Fetching page 4...\n",
      "Fetching page 5...\n",
      "Fetching page 6...\n",
      "Fetching page 7...\n",
      "Fetching page 8...\n",
      "Fetching page 9...\n",
      "Fetching page 10...\n",
      "Fetching page 11...\n",
      "Fetching page 12...\n",
      "Fetching page 13...\n",
      "Fetching page 14...\n",
      "Fetching page 15...\n",
      "Fetching page 16...\n",
      "Fetching page 17...\n",
      "Fetching page 18...\n",
      "Fetching page 19...\n",
      "Fetching page 20...\n",
      "Fetching page 21...\n",
      "Fetching page 22...\n",
      "Fetching page 23...\n",
      "Fetching page 24...\n",
      "Fetching page 25...\n",
      "Fetching page 26...\n",
      "Fetching page 27...\n",
      "Fetching page 28...\n",
      "Fetching page 29...\n",
      "Fetching page 30...\n",
      "Fetching page 31...\n",
      "Fetching page 32...\n",
      "Fetching page 33...\n",
      "Fetching page 34...\n",
      "Fetching page 35...\n",
      "Fetching page 36...\n",
      "Fetching page 37...\n",
      "Fetching page 38...\n",
      "Fetching page 39...\n",
      "Fetching page 40...\n",
      "Fetching page 41...\n",
      "Fetching page 42...\n",
      "Fetching page 43...\n",
      "Fetching page 44...\n",
      "Fetching page 45...\n",
      "Fetching page 46...\n",
      "Fetching page 47...\n",
      "Fetching page 48...\n",
      "Fetching page 49...\n",
      "Fetching page 50...\n",
      "Fetching page 51...\n",
      "Fetching page 52...\n",
      "Fetching page 53...\n",
      "Fetching page 54...\n",
      "Fetching page 55...\n",
      "Fetching page 56...\n",
      "Fetching page 57...\n",
      "Fetching page 58...\n",
      "Fetching page 59...\n",
      "Fetching page 60...\n",
      "Fetching page 61...\n",
      "Fetching page 62...\n",
      "Fetching page 63...\n",
      "Fetching page 64...\n",
      "Fetching page 65...\n",
      "Fetching page 66...\n",
      "Fetching page 67...\n",
      "Fetching page 68...\n",
      "Fetching page 69...\n",
      "Fetching page 70...\n",
      "Fetching page 71...\n",
      "Fetching page 72...\n",
      "Fetching page 73...\n",
      "Fetching page 74...\n",
      "Fetching page 75...\n",
      "Fetching page 76...\n",
      "Fetching page 77...\n",
      "Fetching page 78...\n",
      "Fetching page 79...\n",
      "Fetching page 80...\n",
      "Fetching page 81...\n",
      "Fetching page 82...\n",
      "Fetching page 83...\n",
      "Fetching page 84...\n",
      "Fetching page 85...\n",
      "Fetching page 86...\n",
      "Fetching page 87...\n",
      "Fetching page 88...\n",
      "Fetching page 89...\n",
      "Fetching page 90...\n",
      "Fetching page 91...\n",
      "Fetching page 92...\n",
      "Fetching page 93...\n",
      "Fetching page 94...\n",
      "Fetching page 95...\n",
      "Fetching page 96...\n",
      "Fetching page 97...\n",
      "Fetching page 98...\n",
      "Fetching page 99...\n",
      "Fetching page 100...\n",
      "Reached max_pages limit of 100.\n",
      "Total patterns collected: 10000\n"
     ]
    }
   ],
   "source": [
    "sweaters = search_patterns(\"sweater\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b5bb052",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweaters.to_csv(\"sweaters.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
