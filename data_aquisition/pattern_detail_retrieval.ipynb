{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3447218",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "import pandas as pd\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "BASE_URL = \"https://api.ravelry.com\"\n",
    "\n",
    "# The os.getenv() calls will now find the variables loaded from your .env file\n",
    "RAVELRY_ACCESS_KEY = os.getenv('RAVELRY_ACCESS_KEY')\n",
    "RAVELRY_PERSONAL_KEY = os.getenv('RAVELRY_PERSONAL_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40d1bb58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Access Key Loaded: read-d4086974ad193fe02828dd97c21b9560\n",
      "Personal Key Loaded: Eq5JjrVDcMu4Ji01Y2aQ9bMh4gtUpr1JoSYsG7Ri\n"
     ]
    }
   ],
   "source": [
    "# --- ADD THIS DEBUGGING CODE ---\n",
    "print(f\"Access Key Loaded: {RAVELRY_ACCESS_KEY}\")\n",
    "print(f\"Personal Key Loaded: {RAVELRY_PERSONAL_KEY}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1fb47669",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_categories(category_data):\n",
    "    \"\"\"\n",
    "    Extracts all parent categories from a potentially nested category structure.\n",
    "    \"\"\"\n",
    "    all_categories = set()\n",
    "    \n",
    "    # --- START: ADD THIS NEW LOGIC ---\n",
    "    \n",
    "    # 1. Check if the input is a list and not empty\n",
    "    if isinstance(category_data, list) and category_data:\n",
    "        # If it is, use the first item in the list as our starting point\n",
    "        current_level = category_data[0]\n",
    "    # 2. Check if the input is already a dictionary\n",
    "    elif isinstance(category_data, dict):\n",
    "        # If so, use it directly\n",
    "        current_level = category_data\n",
    "    # 3. Otherwise, we can't process it\n",
    "    else:\n",
    "        return [] # Return an empty list if data is invalid\n",
    "        \n",
    "    # --- END: NEW LOGIC ---\n",
    "    \n",
    "    # Your existing, corrected loop will now work correctly\n",
    "    while isinstance(current_level, dict):\n",
    "        if 'name' in current_level:\n",
    "            all_categories.add(current_level['name'])\n",
    "        current_level = current_level.get('parent')\n",
    "        \n",
    "    return list(all_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5dddb348",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_permalinks(attribute_list):\n",
    "    \"\"\"\n",
    "    Extracts the 'permalink' value from each dictionary in a list.\n",
    "    \"\"\"\n",
    "    if not isinstance(attribute_list, list):\n",
    "        return [] # Return empty list if input is not a list\n",
    "        \n",
    "    return [item['permalink'] for item in attribute_list if 'permalink' in item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3fa5b358",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pattern_details(pattern_id):\n",
    "    \"\"\"\n",
    "    Fetches details for a given pattern ID, now with rate limiting.\n",
    "    \"\"\"\n",
    "    # --- RATE LIMITING ---\n",
    "    # Wait for a short duration *before* each request in a loop.\n",
    "    # A smaller wait is okay here as it's part of a larger process.\n",
    "    time.sleep(0.5) \n",
    "    \n",
    "    details_url = f\"https://api.ravelry.com/patterns/{pattern_id}.json\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(details_url, auth=(RAVELRY_ACCESS_KEY, RAVELRY_PERSONAL_KEY))\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        details_data = response.json()\n",
    "        pattern_data = details_data.get('pattern', {})\n",
    "        \n",
    "        # ... (the rest of your extraction logic remains the same)\n",
    "        craft = pattern_data.get('craft', {})['name']\n",
    "        attributes = pattern_data.get('pattern_attributes', [])\n",
    "        difficulty_average = pattern_data.get('difficulty_average', None)\n",
    "        downloadable = pattern_data.get('downloadable', False)\n",
    "        gauge = pattern_data.get('gauge', None)\n",
    "        gauge_divisor = pattern_data.get('gauge_divisor', None)\n",
    "        gauge_pattern = pattern_data.get('gauge_pattern', None)\n",
    "        # Added .get() for safety on nested dictionaries\n",
    "        pattern_type = pattern_data.get('pattern_type', {}).get('permalink')\n",
    "        yarn_weight = pattern_data.get('yarn_weight', {}).get('name')\n",
    "        projects_count = pattern_data.get('projects_count', 0)\n",
    "        rating_average = pattern_data.get('rating_average', None)\n",
    "        sizes_available = pattern_data.get('sizes_available', \"\")\n",
    "        photos = pattern_data.get('photos', [])\n",
    "\n",
    "        return {\n",
    "            'Craft': craft, \n",
    "            'Attributes': extract_permalinks(attributes), \n",
    "            'Gauge': gauge, \n",
    "            'Difficulty Average': difficulty_average, \n",
    "            'Downloadable': downloadable, \n",
    "            'Gauge Divisor': gauge_divisor, \n",
    "            'Gauge Pattern': gauge_pattern, \n",
    "            'Pattern Type': pattern_type, \n",
    "            'Yarn Weight': yarn_weight, \n",
    "            'Projects Count': projects_count, \n",
    "            'Rating Average': rating_average, \n",
    "            'Sizes Available': sizes_available,\n",
    "            'Photos': photos\n",
    "        }\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Could not fetch data for pattern ID {pattern_id}: {e}\")\n",
    "        return {\n",
    "            'Craft': None, 'Attributes': [], 'Gauge': None, \n",
    "            'Difficulty Average': None, 'Downloadable': False, 'Gauge Divisor': None, \n",
    "            'Gauge Pattern': None, 'Pattern Type': None, 'Yarn Weight': None, \n",
    "            'Projects Count': 0, 'Rating Average': None, 'Sizes Available': \"\"\n",
    "        }\n",
    "\n",
    "def add_details_to_df(df):\n",
    "    \"\"\"\n",
    "    Applies the single, efficient function to the DataFrame to create\n",
    "    three new columns from the returned data.\n",
    "    \"\"\"\n",
    "    # Using .apply with a lambda function that returns a pandas Series\n",
    "    # is an efficient way to create multiple columns at once.\n",
    "    details = df['ID'].apply(lambda pid: pd.Series(get_pattern_details(pid)))\n",
    "    \n",
    "    # Join the newly created columns back to the original DataFrame\n",
    "    return df.join(details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c8011ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load your main CSV file\n",
    "sweaters_df = pd.read_csv(\"sweaters_v3.csv\")\n",
    "\n",
    "# --- Define Chunking Parameters ---\n",
    "chunk_size = 500  # Process 500 rows at a time\n",
    "output_dir = \"processed_chunks\" # A folder to store intermediate files\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Split the dataframe into a list of smaller dataframes\n",
    "list_of_dfs = np.array_split(sweaters_df, len(sweaters_df) // chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3bbf6738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏩ Chunk 1 already processed. Skipping.\n",
      "⏩ Chunk 2 already processed. Skipping.\n",
      "⏩ Chunk 3 already processed. Skipping.\n",
      "⏩ Chunk 4 already processed. Skipping.\n",
      "⏩ Chunk 5 already processed. Skipping.\n",
      "⏩ Chunk 6 already processed. Skipping.\n",
      "⏩ Chunk 7 already processed. Skipping.\n",
      "⏩ Chunk 8 already processed. Skipping.\n",
      "⏩ Chunk 9 already processed. Skipping.\n",
      "⏩ Chunk 10 already processed. Skipping.\n",
      "⏩ Chunk 11 already processed. Skipping.\n",
      "⏩ Chunk 12 already processed. Skipping.\n",
      "⏩ Chunk 13 already processed. Skipping.\n",
      "⏩ Chunk 14 already processed. Skipping.\n",
      "⏩ Chunk 15 already processed. Skipping.\n",
      "⏩ Chunk 16 already processed. Skipping.\n",
      "⏩ Chunk 17 already processed. Skipping.\n",
      "⏩ Chunk 18 already processed. Skipping.\n",
      "⏩ Chunk 19 already processed. Skipping.\n",
      "⏩ Chunk 20 already processed. Skipping.\n",
      "⏩ Chunk 21 already processed. Skipping.\n",
      "⏩ Chunk 22 already processed. Skipping.\n",
      "⏩ Chunk 23 already processed. Skipping.\n",
      "⏩ Chunk 24 already processed. Skipping.\n",
      "⏩ Chunk 25 already processed. Skipping.\n",
      "⏩ Chunk 26 already processed. Skipping.\n",
      "⏩ Chunk 27 already processed. Skipping.\n",
      "⏩ Chunk 28 already processed. Skipping.\n",
      "⏩ Chunk 29 already processed. Skipping.\n",
      "⏩ Chunk 30 already processed. Skipping.\n",
      "⏩ Chunk 31 already processed. Skipping.\n",
      "⏩ Chunk 32 already processed. Skipping.\n",
      "⏩ Chunk 33 already processed. Skipping.\n",
      "⏩ Chunk 34 already processed. Skipping.\n",
      "⏩ Chunk 35 already processed. Skipping.\n",
      "⏩ Chunk 36 already processed. Skipping.\n",
      "⏩ Chunk 37 already processed. Skipping.\n",
      "⏩ Chunk 38 already processed. Skipping.\n",
      "⏩ Chunk 39 already processed. Skipping.\n",
      "⏩ Chunk 40 already processed. Skipping.\n",
      "⏩ Chunk 41 already processed. Skipping.\n",
      "⏩ Chunk 42 already processed. Skipping.\n",
      "⏩ Chunk 43 already processed. Skipping.\n",
      "⏩ Chunk 44 already processed. Skipping.\n",
      "⏩ Chunk 45 already processed. Skipping.\n",
      "⏩ Chunk 46 already processed. Skipping.\n",
      "⏩ Chunk 47 already processed. Skipping.\n",
      "⏩ Chunk 48 already processed. Skipping.\n",
      "⏩ Chunk 49 already processed. Skipping.\n",
      "⏩ Chunk 50 already processed. Skipping.\n",
      "⏩ Chunk 51 already processed. Skipping.\n",
      "⏩ Chunk 52 already processed. Skipping.\n",
      "⏩ Chunk 53 already processed. Skipping.\n",
      "⏩ Chunk 54 already processed. Skipping.\n",
      "⏩ Chunk 55 already processed. Skipping.\n",
      "⏩ Chunk 56 already processed. Skipping.\n",
      "⏩ Chunk 57 already processed. Skipping.\n",
      "⏩ Chunk 58 already processed. Skipping.\n",
      "⏩ Chunk 59 already processed. Skipping.\n",
      "⏩ Chunk 60 already processed. Skipping.\n",
      "⏩ Chunk 61 already processed. Skipping.\n",
      "⏩ Chunk 62 already processed. Skipping.\n",
      "⏩ Chunk 63 already processed. Skipping.\n",
      "⏩ Chunk 64 already processed. Skipping.\n",
      "⏩ Chunk 65 already processed. Skipping.\n",
      "⏩ Chunk 66 already processed. Skipping.\n",
      "⏩ Chunk 67 already processed. Skipping.\n",
      "⏩ Chunk 68 already processed. Skipping.\n",
      "⏩ Chunk 69 already processed. Skipping.\n",
      "⏩ Chunk 70 already processed. Skipping.\n",
      "⏩ Chunk 71 already processed. Skipping.\n",
      "--- Processing chunk 72 of 79 ---\n",
      "✅ Successfully saved chunk 72 to processed_chunks/sweaters_chunk_72.csv\n",
      "--- Processing chunk 73 of 79 ---\n",
      "✅ Successfully saved chunk 73 to processed_chunks/sweaters_chunk_73.csv\n",
      "--- Processing chunk 74 of 79 ---\n",
      "✅ Successfully saved chunk 74 to processed_chunks/sweaters_chunk_74.csv\n",
      "--- Processing chunk 75 of 79 ---\n",
      "✅ Successfully saved chunk 75 to processed_chunks/sweaters_chunk_75.csv\n",
      "--- Processing chunk 76 of 79 ---\n",
      "✅ Successfully saved chunk 76 to processed_chunks/sweaters_chunk_76.csv\n",
      "--- Processing chunk 77 of 79 ---\n",
      "✅ Successfully saved chunk 77 to processed_chunks/sweaters_chunk_77.csv\n",
      "--- Processing chunk 78 of 79 ---\n",
      "✅ Successfully saved chunk 78 to processed_chunks/sweaters_chunk_78.csv\n",
      "--- Processing chunk 79 of 79 ---\n",
      "✅ Successfully saved chunk 79 to processed_chunks/sweaters_chunk_79.csv\n",
      "\\n--- All chunks processed! ---\n"
     ]
    }
   ],
   "source": [
    "# Loop through each chunk, process it, and save it\n",
    "for i, chunk_df in enumerate(list_of_dfs):\n",
    "    \n",
    "    # Define the expected output path for this chunk\n",
    "    chunk_number = i + 1\n",
    "    output_path = os.path.join(output_dir, f\"sweaters_chunk_{chunk_number}.csv\")\n",
    "\n",
    "    # --- THIS IS THE NEW LOGIC ---\n",
    "    # Check if this chunk's output file already exists\n",
    "    if os.path.exists(output_path):\n",
    "        print(f\"⏩ Chunk {chunk_number} already processed. Skipping.\")\n",
    "        continue  # Move to the next iteration of the loop\n",
    "\n",
    "    # --- The original processing logic runs only if the file doesn't exist ---\n",
    "    print(f\"--- Processing chunk {chunk_number} of {len(list_of_dfs)} ---\")\n",
    "    \n",
    "    try:\n",
    "        # Apply your existing function to fetch details for this chunk\n",
    "        processed_chunk = add_details_to_df(chunk_df)\n",
    "        \n",
    "        # Save the processed chunk to its own CSV file\n",
    "        processed_chunk.to_csv(output_path, index=False)\n",
    "        \n",
    "        print(f\"✅ Successfully saved chunk {chunk_number} to {output_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ An error occurred on chunk {chunk_number}: {e}\")\n",
    "        print(\"Stopping process. Please check the error and re-run the script to resume.\")\n",
    "        break # Stop the script if an error occurs\n",
    "\n",
    "print(\"\\\\n--- All chunks processed! ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78c8dc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
